
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Recommended Metrics &#8212; BigEarthNet Guide</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ToDo" href="80_todos.html" />
    <link rel="prev" title="Geographical Patch distribution" href="25_patch-distribution.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">BigEarthNet Guide</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="05_basics.html">
   Remote Sensing Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_data-source.html">
   Data Source
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_labels.html">
   BigEarthNet Labels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20_raw-data.html">
   Raw BigEarthNet Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21_visualizations.html">
   Visualizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25_patch-distribution.html">
   Geographical Patch distribution
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Recommended Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="80_todos.html">
   ToDo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="90_libraries.html">
   Helpful Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   Further references
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/kai-tub/ben-docs"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/50_metrics.mipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.mipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-for-evaluation-of-multi-label-classification">
   mAP for Evaluation of Multi-Label Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-of-f1-score-failure-of-describing-predictive-power">
     Example of F1 score failure of describing predictive power
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Recommended Metrics</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-for-evaluation-of-multi-label-classification">
   mAP for Evaluation of Multi-Label Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-of-f1-score-failure-of-describing-predictive-power">
     Example of F1 score failure of describing predictive power
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="recommended-metrics">
<h1>Recommended Metrics<a class="headerlink" href="#recommended-metrics" title="Permalink to this headline">#</a></h1>
<p>Here, we would like to provide a short overview of different metrics and give our opionated recommendations.</p>
<ul class="simple">
<li><p>For multi-label classification: mAP</p></li>
<li><p>For image-retrieval: NDCG</p></li>
</ul>
<p>Before we dive into the recommended metrics, a short reminder:
Almost all metrics (Precision, Recall, AP, mAP) are defined <em>differently</em> in the multi-label vs image-retrieval domain!</p>
<p>In the multi-label classification we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
    \text{Precision} &amp;= \frac{\text{TP}}{\text{TP} + \text{FP}} \\
    \text{Recall} &amp;= \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\end{align*}
\end{split}\]</div>
<p>with:</p>
<ul class="simple">
<li><p>TP: True positive</p></li>
<li><p>FP: False positive</p></li>
<li><p>FN: False negative</p></li>
</ul>
<p>For more information see the <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">wikipedia definition</a> under the section titled <code class="docutils literal notranslate"><span class="pre">Definition</span> <span class="pre">(classification</span> <span class="pre">context)</span></code>!</p>
<div class="admonition-todo admonition">
<p class="admonition-title">ToDo</p>
<p>We will add more information about image retrieval and the recommended metrics in the future.</p>
</div>
<section id="map-for-evaluation-of-multi-label-classification">
<h2>mAP for Evaluation of Multi-Label Classification<a class="headerlink" href="#map-for-evaluation-of-multi-label-classification" title="Permalink to this headline">#</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Thank you Tom for writing this section!</p>
</div>
<p>A common way of summarizing evaluation performance in terms of Precision and Recall within a single term is the F1 score. The F1 score can be interpreted as a harmonic mean of Precision and Recall, where an F1 score of 1 is related to optimal performance and a F1 score of 0 to worst performance, respectively. The relative contribution of Precision and Recall to the F1 score are equal. If multiple classes are considered (e.g., in Multi-Class Classification tasks like <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10/100</a> or <a class="reference external" href="https://www.image-net.org/">ImageNet</a>) the F1 score is usually either evaluated class-based or globally, considered as macro or micro averaging (further reading on macro and micro averaging can be seen in <a class="reference external" href="https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f">this micro/macro tutorial on Medium</a>).</p>
<p>Typically, when evaluating models that were trained on a Multi-Label Classification task like <a class="reference external" href="https://bigearth.net/">BigEarthNet</a> or the multi-label version of the <a class="reference external" href="https://cocodataset.org/#home">COCO dataset</a>, these evaluation procedures are adopted. However, due to the requirement of being able to predict multiple classes at once, the softmax function that is applied in classification tasks with a single label such as Binary Classfication or Multi-Class Classification serving as a probabilty normalizer, has to be exchanged by a sigmoid function that enables multiple (positive) predictions at the same time.</p>
<p>The drawback of the sigmoid function is that the final predictions per class become disentangled. It may happen that the optimal decision boundary for some classes fall below 0.5. A consequence is that the F1 score (as well as Precision and Recall) that rely on a fixed prediction threshold falsely indicate bad performance for that class, while the inherent predictive power for that class is good, but just at a different scale. A solution to overcome the artefact of class-wise disentangled probabilities provides the metric <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html">(mean) Average Precision</a>. It refers to the area under the <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#in-multi-label-settings">Precision-Recall curve</a> and is a more holistic approach for describing predictive power as it does not depend on a probability threshold. Likewise, as other metrics, the mAP metric can be either computed by micro or macro averaging.</p>
<p>In many multi-label scenarios, F1 score and mAP will highly correlate. But there exist scenarios in which the F1 score looses its descriptivity for predictive power due to scaling problems (e.g., under some type of multi-label noise), whereas the mAP score maintains its descriptivity. An example can be found below in the <a class="reference internal" href="#f1-fail"><span class="std std-ref">example section</span></a>. Therefore, it is recommended to always report both F1 score and mAP under micro and macro averaging.</p>
<p>SciKit-Learn provides a predefined function <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html">average_prevision_score</a> to calculate the AP and/or mAP.
By setting <code class="docutils literal notranslate"><span class="pre">average=None</span></code> the AP is calculated for each class and can be used in the multi-label classification scenario to better understand what class might be misbehaving.
The mAP is then defined by the specific averaging procedure (<code class="docutils literal notranslate"><span class="pre">macro</span></code>, <code class="docutils literal notranslate"><span class="pre">micro</span></code>, <code class="docutils literal notranslate"><span class="pre">samples</span></code>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Do not forget to mention what averaging procedure was used!
Otherwise the reported metric has no value, as there is no way of knowing what the metric is <em>measuring</em>.</p>
</div>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h3>
<p>It is not uncommon that the optimal decision boundary of a multi-label classification model is <em>not</em> 0.5 for all classes.
Metrics that rely on a fixed prediction threshold (Precision/Recall/F1) may report bad performance, even though the model can predict the class, just not at the threshold of 0.5.
AP/mAP do not rely on any threshold and give <em>robust</em> estimates of the predictive power of the model.
When reporting the mAP the averaging procedure has to be included.</p>
</section>
<section id="example-of-f1-score-failure-of-describing-predictive-power">
<span id="f1-fail"></span><h3>Example of F1 score failure of describing predictive power<a class="headerlink" href="#example-of-f1-score-failure-of-describing-predictive-power" title="Permalink to this headline">#</a></h3>
<p>The following code shows an failure of the F1 score.
For simplicity, a single label classification task is reviewed but the argumentation also holds for multi-label classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">424</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_print_scores</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:       </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1-Score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)))</span>


<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Perfect Probability scaling:</p>
<p>Our awesome prediction model samples from a uniform distribution [0, 1].
In other words, the model does output over the entire range of possible probability values [0, 1].
We call this “perfect” probability scaling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span>
<span class="n">_print_scores</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AP:       0.45
F1-Score: 0.33
</pre></div>
</div>
</div>
</div>
<p>Bad Probability scaling:</p>
<p>Here, our random model is predicting values in the range [0, 0.6], which is equal to bad probability scaling.
This will probably not happen for <em>all</em> classes in the multi-label classification scenario, but might happen to some!</p>
<p>We multiply the old <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> value by a value &lt; 1 to show that the F1-score will differ, while the AP will remain constant.
The AP does not require setting/tuning a threshold!
Tuning the threshold could also be seen as adjusting to the output probability scale of the model.
The specific value of the F1-score will differ; it doesn’t matter if the value itself becomes larger/smaller than the previously shown score in the ‘perfect probability scaling’ example.</p>
<p>The important take-away is that the predictive power of the model is the same.
Only the output probability scale has changed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_bad</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">*</span> <span class="mf">0.6</span>
<span class="n">_print_scores</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred_bad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AP:       0.45
F1-Score: 0.00
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="25_patch-distribution.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Geographical Patch distribution</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="80_todos.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ToDo</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Kai Norman Clasen<br/>
  
      &copy; Copyright 2022, Kai Norman Clasen.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>